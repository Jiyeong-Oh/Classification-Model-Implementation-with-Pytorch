{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jiyeong-Oh/Classification-Model-Implementation-with-Pytorch/blob/main/Model_Implementation_Report_with_Pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTeFTONui1yp"
      },
      "source": [
        "# **Library Import & Settings**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ACB282mhTYQX"
      },
      "outputs": [],
      "source": [
        "import sklearn\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from sklearn.datasets import fetch_openml\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from numpy import random as rd\n",
        "import torch.nn.functional as F\n",
        "from collections import Counter\n",
        "device = 'cuda:0'\n",
        "\n",
        "# Data Loading\n",
        "mnist = fetch_openml('mnist_784',cache = False)\n",
        "X = mnist.data.astype('float32')\n",
        "y = mnist.target.astype('int64')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKpCD_1jGfhG"
      },
      "source": [
        "# **1. Data Split Function**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8N_0282D2sWR"
      },
      "outputs": [],
      "source": [
        "def data_split(attr, tar): # attr: features, tar: target label\n",
        "  # Making Dictionary with Answer Label as Key and its Indices as Value\n",
        "  # so that we can split them into train/validation/test with the same digit propotion.\n",
        "  y_dict = dict(zip([i for i in tar.unique()], [list(tar[tar.values==i].index) for i in tar.unique()]))\n",
        "  y_dict = dict(zip([i for i in tar.unique()], [list(np.random.choice(y_dict[i], len(y_dict[i]), replace = False)) for i in y_dict.keys()])) # Shuffling\n",
        "\n",
        "  train_idx=[]\n",
        "  val_idx=[]\n",
        "  test_idx=[]\n",
        "  train_tmp = [y_dict[i][:int(round(len(y_dict[i])*0.7,-1))] for i in sorted(y_dict.keys())] # Trimming train set (70%)\n",
        "  val_tmp = [y_dict[i][int(round(len(y_dict[i])*0.7,-1)):int(round(len(y_dict[i])*0.85,-1))] for i in sorted(y_dict.keys())] #Ttrimming validation set (15%)\n",
        "  test_tmp = [y_dict[i][int(round(len(y_dict[i])*0.85,-1)):] for i in sorted(y_dict.keys())] # Trimming test set (15%)\n",
        "\n",
        "  for i in range(len(sorted(y_dict.keys()))):\n",
        "    train_idx+=train_tmp[i]\n",
        "    val_idx+=val_tmp[i]\n",
        "    test_idx+=test_tmp[i]\n",
        "\n",
        "  # Converting into Tensor Type & Normalizing\n",
        "  X_train, y_train = torch.tensor(attr.loc[train_idx].to_numpy())/255, torch.tensor(tar.loc[train_idx].to_numpy())\n",
        "  X_val, y_val = torch.tensor(attr.loc[val_idx].to_numpy())/255, torch.tensor(tar.loc[val_idx].to_numpy())\n",
        "  X_test, y_test = torch.tensor(attr.loc[test_idx].to_numpy())/255, torch.tensor(tar.loc[test_idx].to_numpy())\n",
        "\n",
        "  return X_train, y_train, X_val, y_val, X_test, y_test"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "TgTEqV0Dz5_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtNbo8XGg_YB"
      },
      "source": [
        "# **2. Binary Classification via soft-margin SVM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lSVeGItXXsx6"
      },
      "outputs": [],
      "source": [
        "###############################\n",
        "# Remote Controller\n",
        "###############################\n",
        "epochs_num = 10\n",
        "batch_size = 64\n",
        "optimizer_choose = 'Adam' # another option can be 'Adam'.\n",
        "learning_rate = 0.01\n",
        "gamma = 0.001 # hyperparameter that controls how much slack is allowed. 0 to infinite.\n",
        "final_test = True # only true for final test accuracy\n",
        "###############################\n",
        "\n",
        "\n",
        "# Model Class Defining\n",
        "class SoftMargin_SVM(nn.Module):\n",
        "  def __init__(self, input_dim, output_dim):\n",
        "    super(SoftMargin_SVM, self).__init__()\n",
        "    self.module = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    y_pred = self.module(x)\n",
        "    return y_pred\n",
        "\n",
        "def softMarginSvm(X, y, epochs_num=epochs_num, batch_size=batch_size, optimizer_choose=optimizer_choose, learning_rate=learning_rate, gamma=gamma, final_test=final_test):  \n",
        "  # Data Splitting\n",
        "  y_two_three = y[(y.values==2) | (y.values==3)]\n",
        "  for i in range(len(y_two_three)):\n",
        "    if y_two_three.iloc[i] == 2:\n",
        "      y_two_three.iloc[i] = -1 # converting raw labels into -1 & 1\n",
        "    else:\n",
        "      y_two_three.iloc[i] = 1\n",
        "  X_two_three = X.loc[y_two_three.index]\n",
        "  X_train, y_train, X_val, y_val, X_test, y_test = data_split(X_two_three, y_two_three)\n",
        "  print(X_train.shape)\n",
        "  data_size = len(X_train)\n",
        "\n",
        "  # Model Training\n",
        "  model = SoftMargin_SVM(X_train.shape[1], 1)\n",
        "  if optimizer_choose == 'SGD':\n",
        "    optimizer = optim.SGD(model.parameters(), lr = learning_rate)\n",
        "  elif optimizer_choose == 'Adam':\n",
        "    optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
        "  model.train()\n",
        "\n",
        "  def hinge_loss(y_actual, y_pred):\n",
        "    return torch.clamp(1-y_pred*y_actual, min=0)\n",
        "\n",
        "  for epoch in (range(epochs_num)):\n",
        "    cost = 0\n",
        "    batch = torch.randperm(data_size)\n",
        "    for i in range(0, data_size, batch_size):\n",
        "      input = X_train[batch[i:i+batch_size]]\n",
        "      answer = y_train[batch[i:i+batch_size]]\n",
        "      optimizer.zero_grad()\n",
        "      prediction = model(input)\n",
        "      answer = answer.reshape(-1,1)\n",
        "      loss = gamma*torch.norm(list(model.parameters())[0])/2 + torch.mean(hinge_loss(answer, prediction))\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      cost += loss.data.numpy()\n",
        "\n",
        "      # Validation Check\n",
        "      model.eval()\n",
        "      with torch.no_grad():\n",
        "        val_prediction = model(X_val)\n",
        "        acc_count = 0\n",
        "        for i in range(len(val_prediction)):\n",
        "          if val_prediction[i] >= 0:\n",
        "            val_prediction[i] = 1\n",
        "          else:\n",
        "            val_prediction[i] = -1\n",
        "          if val_prediction[i]==y_val[i]:\n",
        "            acc_count+=1\n",
        "    # Train Loss & Validation Accuracy\n",
        "    print('Epoch Train Loss {}: {: .2f}'.format(epoch,cost),  \", Validation Accuracy: {: .2f}\".format(acc_count/len(y_val)))\n",
        "\n",
        "\n",
        "  # Final Test Accuracy Check\n",
        "  if final_test==True:\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      test_prediction = model(X_test)\n",
        "      acc_count = 0\n",
        "      for i in range(len(test_prediction)):\n",
        "        if test_prediction[i] >= 0:\n",
        "          test_prediction[i] = 1\n",
        "        else:\n",
        "          test_prediction[i] = -1\n",
        "        if test_prediction[i]==y_test[i]:\n",
        "          acc_count+=1\n",
        "      print(\"Final Test Accuracy: {: .2f}\".format(acc_count/len(y_test)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9sxsDZkKX5PU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fada7ffb-2cc6-4ed5-cf24-8af468b0142e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([9890, 784])\n",
            "Epoch Train Loss 0:  17.16 , Validation Accuracy:  0.96\n",
            "Epoch Train Loss 1:  11.84 , Validation Accuracy:  0.97\n",
            "Epoch Train Loss 2:  11.01 , Validation Accuracy:  0.97\n",
            "Epoch Train Loss 3:  10.97 , Validation Accuracy:  0.97\n",
            "Epoch Train Loss 4:  10.38 , Validation Accuracy:  0.97\n",
            "Epoch Train Loss 5:  10.58 , Validation Accuracy:  0.97\n",
            "Epoch Train Loss 6:  9.88 , Validation Accuracy:  0.96\n",
            "Epoch Train Loss 7:  10.96 , Validation Accuracy:  0.97\n",
            "Epoch Train Loss 8:  10.50 , Validation Accuracy:  0.97\n",
            "Epoch Train Loss 9:  9.78 , Validation Accuracy:  0.97\n",
            "Final Test Accuracy:  0.98\n"
          ]
        }
      ],
      "source": [
        "softMarginSvm(X, y, epochs_num, batch_size, optimizer_choose, learning_rate, gamma, final_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21Qf5k60pNh8"
      },
      "source": [
        "# **3. Binary Classification via MLP**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hj5mU_LxkTF9"
      },
      "outputs": [],
      "source": [
        "###############################\n",
        "# Remote Controller\n",
        "###############################\n",
        "epochs_num = 5\n",
        "batch_size = 32\n",
        "optimizer_choose = 'Adam' # another option can be 'Adam'.\n",
        "learning_rate = 0.01\n",
        "final_test = True # only true for final test accuracy\n",
        "###############################\n",
        "\n",
        "\n",
        "# Model Class Defining\n",
        "class MLP_binary(nn.Module):\n",
        "  def __init__(self, input_dim, output_dim):\n",
        "    super(MLP_binary, self).__init__()\n",
        "    self.module1 = nn.Linear(input_dim, 256, bias=True) #input layer -> hidden layer1\n",
        "    self.module2 = nn.Linear(256, 256, bias=True) #hidden layer1 -> hidden layer2\n",
        "    self.module3 = nn.Linear(256, output_dim, bias=True) #hidden layer 2 -> output layer\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.module1(x))\n",
        "    x = F.relu(self.module2(x))\n",
        "    x = self.module3(x)\n",
        "    y_pred = torch.sigmoid(x) # activation function for binary (0~1)\n",
        "    return y_pred\n",
        "\n",
        "def binaryMLP(X, y, epochs_num=epochs_num, batch_size=batch_size, optimizer_choose=optimizer_choose, learning_rate=learning_rate, final_test=final_test):\n",
        "  # Data Splitting\n",
        "  y_two_three = y[(y.values==2) | (y.values==3)]\n",
        "  for i in range(len(y_two_three)):\n",
        "    if y_two_three.iloc[i] == 2:\n",
        "      y_two_three.iloc[i] = 0\n",
        "    else:\n",
        "      y_two_three.iloc[i] = 1\n",
        "  X_two_three = X.loc[y_two_three.index]\n",
        "  X_train, y_train, X_val, y_val, X_test, y_test = data_split(X_two_three, y_two_three)\n",
        "  data_size = len(X_train)\n",
        "\n",
        "  # Model Training\n",
        "  model = MLP_binary(X_train.shape[1], 1)\n",
        "  if optimizer_choose == 'SGD':\n",
        "    optimizer = optim.SGD(model.parameters(), lr = learning_rate)\n",
        "  elif optimizer_choose == 'Adam':\n",
        "    optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
        "  model.train()\n",
        "\n",
        "  criterion = nn.BCELoss() # loss function: cross entropy for binary classification\n",
        "  for epoch in range(epochs_num):\n",
        "    cost = 0\n",
        "    batch = torch.randperm(data_size) # batch mixing\n",
        "    for i in range(0, data_size, batch_size):\n",
        "      input = X_train[batch[i:i+batch_size]]\n",
        "      answer = y_train[batch[i:i+batch_size]]\n",
        "      optimizer.zero_grad()\n",
        "      prediction = model(input)\n",
        "      answer = answer.reshape(-1,1)\n",
        "      loss = criterion(prediction.to(torch.float32), answer.to(torch.float32))\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      cost += loss.data.numpy()\n",
        "\n",
        "    # Validation Check\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      val_prediction = model(X_val)\n",
        "      acc_count = 0\n",
        "      for i in range(len(val_prediction)):\n",
        "        if val_prediction[i] >= 0.5:\n",
        "          val_prediction[i] = 1\n",
        "        else:\n",
        "          val_prediction[i] = 0\n",
        "        if val_prediction[i]==y_val[i]:\n",
        "          acc_count+=1\n",
        "      # Train Loss & Validation Accuracy\n",
        "      print('Epoch Train Loss {}: {: .2f}'.format(epoch,cost),  \", Validation Accuracy: {: .2f}\".format(acc_count/len(y_val)))\n",
        "\n",
        "  # Final Test Accuracy Check\n",
        "  if final_test==True:\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      test_prediction = model(X_test)\n",
        "      acc_count = 0\n",
        "      for i in range(len(test_prediction)):\n",
        "        if test_prediction[i] >= 0.5:\n",
        "          test_prediction[i] = 1\n",
        "        else:\n",
        "          test_prediction[i] = 0\n",
        "        if test_prediction[i]==y_test[i]:\n",
        "          acc_count+=1\n",
        "      print(\"Final Test Accuracy: {: .2f}\".format(acc_count/len(y_test)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V0WdfZvuU1RP",
        "outputId": "7fbf367c-6e1d-4ee5-f2f2-8636b6a5d70f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch Train Loss 0:  31.39 , Validation Accuracy:  0.99\n",
            "Epoch Train Loss 1:  13.95 , Validation Accuracy:  0.99\n",
            "Epoch Train Loss 2:  10.21 , Validation Accuracy:  0.99\n",
            "Epoch Train Loss 3:  11.75 , Validation Accuracy:  0.99\n",
            "Epoch Train Loss 4:  5.58 , Validation Accuracy:  0.99\n",
            "Final Test Accuracy:  0.99\n"
          ]
        }
      ],
      "source": [
        "binaryMLP(X, y, epochs_num, batch_size, optimizer_choose, learning_rate, final_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTFEm5G14xnb"
      },
      "source": [
        "# **4. Multiclass Classification via MLP**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sr32KBBnpzWP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "outputId": "e58cddcb-905b-477a-c366-0a526004482c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-b4edceece5d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Model Class Defining\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mMLP_multi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMLP_multi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
          ]
        }
      ],
      "source": [
        "###############################\n",
        "# Remote Controller\n",
        "###############################\n",
        "epochs_num = 5\n",
        "batch_size = 64\n",
        "optimizer_choose = 'Adam' # another option can be 'Adam'.\n",
        "learning_rate = 0.0001\n",
        "final_test = True # only true for final test accuracy\n",
        "###############################\n",
        "\n",
        "# Model Class Defining\n",
        "class MLP_multi(nn.Module):\n",
        "  def __init__(self, input_dim, output_dim):\n",
        "    super(MLP_multi, self).__init__()\n",
        "    self.module1 = nn.Linear(input_dim, 256, bias=True) #input layer -> hidden layer1\n",
        "    self.module2 = nn.Linear(256, 256, bias=True) #hidden layer1 -> hidden layer2\n",
        "    self.module3 = nn.Linear(256, output_dim, bias=True) #hidden layer 2 -> output layer. here, the output layer should be the number of digits (10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.module1(x))\n",
        "    x = F.relu(self.module2(x))\n",
        "    y_pred = self.module3(x)\n",
        "    # softmax is gonna be applied when cross entropy loss is calculated\n",
        "    return y_pred\n",
        "\n",
        "def multiMLP(X, y, epochs_num=epochs_num, batch_size=batch_size, optimizer_choose=optimizer_choose, learning_rate=learning_rate, final_test=final_test):\n",
        "  # Data Splitting\n",
        "  X_train, y_train, X_val, y_val, X_test, y_test = data_split(X, y)\n",
        "  data_size = len(X_train)\n",
        "\n",
        "  # Model Training\n",
        "  model = MLP_multi(X_train.shape[1], 10)\n",
        "  if optimizer_choose == 'SGD':\n",
        "    optimizer = optim.SGD(model.parameters(), lr = learning_rate)\n",
        "  elif optimizer_choose == 'Adam':\n",
        "    optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
        "  model.train()\n",
        "  criterion = nn.CrossEntropyLoss() # loss function\n",
        "\n",
        "  for epoch in range(epochs_num):\n",
        "    cost = 0\n",
        "    batch = torch.randperm(data_size) # batch mixing\n",
        "    for i in range(0, data_size, batch_size):\n",
        "      input = X_train[batch[i:i+batch_size]]\n",
        "      answer = y_train[batch[i:i+batch_size]]\n",
        "      optimizer.zero_grad()\n",
        "      prediction = model(input)\n",
        "      loss = criterion(prediction, answer)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      cost += loss.data.numpy()\n",
        "\n",
        "    # Validation Check\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      val_prediction = model(X_val)\n",
        "      acc_count = 0\n",
        "      for i in range(len(val_prediction)):\n",
        "        pred = torch.argmax(val_prediction[i])\n",
        "        if pred==y_val[i]:\n",
        "          acc_count+=1\n",
        "      # Train Loss & Validation Accuracy\n",
        "      print('Epoch Train Loss {}: {: .2f}'.format(epoch,cost),  \", Validation Accuracy: {: .2f}\".format(acc_count/len(y_val)))\n",
        "    \n",
        "  # Final Test Accuracy Check\n",
        "  if final_test==True:\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      test_prediction = model(X_test)\n",
        "      acc_count = 0\n",
        "      for i in range(len(test_prediction)):\n",
        "        pred = torch.argmax(test_prediction[i])\n",
        "        if pred==y_test[i]:\n",
        "          acc_count+=1\n",
        "      print(\"Final Test Accuracy: {: .2f}\".format(acc_count/len(y_test)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J9fOld4bU1W-"
      },
      "outputs": [],
      "source": [
        "multiMLP(X, y, epochs_num, batch_size, optimizer_choose, learning_rate, final_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIObB5ls5FYk"
      },
      "source": [
        "# **5. Multiclass Classification via k-NN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8pvQI7EF5JDK"
      },
      "outputs": [],
      "source": [
        "###############################\n",
        "# Remote Controller\n",
        "###############################\n",
        "k = 10\n",
        "distance_choose = 'L2Norm' #another can be 'L1Norm'\n",
        "final_test = True # only true for final test accuracy\n",
        "###############################\n",
        "\n",
        "def knn(k=k, distance_choose=distance_choose, final_test = final_test):\n",
        "  # Data Splitting\n",
        "  X_train, y_train, X_val, y_val, X_test, y_test = data_split(X, y)\n",
        "\n",
        "  # Validation Check\n",
        "  acc_count = 0\n",
        "  for i in range(len(X_val)):\n",
        "    if distance_choose == 'L2Norm':\n",
        "      distance = torch.norm(X_train - X_val[i], dim=1, p='fro') #L2 norm for distance calculating (Frobenius norm)\n",
        "    elif distance_choose == 'L1Norm':\n",
        "      distance = torch.norm(X_train - X_val[i], dim=1, p=1 ) #L1 norm for distance calculating (nuclear norm)\n",
        "    knn_idx = torch.topk(distance, k, largest=False)[1]\n",
        "    k_neighbors = y_train.numpy()[knn_idx]\n",
        "    pred = Counter(k_neighbors).most_common(1)[0][0]\n",
        "    if pred==y_val[i]:\n",
        "          acc_count+=1\n",
        "  print(\"Validation Accuracy: {: .2f}\".format(acc_count/len(y_val)))\n",
        "\n",
        "  # Final Test Accuracy Check\n",
        "  if final_test==True:\n",
        "    acc_count = 0\n",
        "    for i in range(len(X_test)):\n",
        "      if distance_choose == 'L2Norm':\n",
        "        distance = torch.norm(X_train - X_val[i], dim=1, p='fro') #L2 norm for distance calculating (Frobenius norm)\n",
        "      elif distance_choose == 'L1Norm':\n",
        "        distance = torch.norm(X_train - X_val[i], dim=1, p=1) #L1 norm for distance calculating (nuclear norm)\n",
        "      knn_idx = torch.topk(distance, k, largest=False)[1]\n",
        "      k_neighbors = y_train.numpy()[knn_idx]\n",
        "      pred = Counter(k_neighbors).most_common(1)[0][0]\n",
        "      if pred==y_test[i]:\n",
        "            acc_count+=1\n",
        "    print(\"Final Test Accuracy: {: .2f}\".format(acc_count/len(y_test)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J01S6nebuRzr",
        "outputId": "d3d88c48-0eb7-47f4-da64-68e3c09dcce0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy:  0.97\n",
            "Final Test Accuracy:  0.96\n"
          ]
        }
      ],
      "source": [
        "knn(k, distance_choose, final_test)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Model Implementation Report with Pytorch.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMSvUTK5A8TtQGuZStqcZrf",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}